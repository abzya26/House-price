import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.metrics import mean_squared_error
from sklearn.preprocessing import StandardScaler
from xgboost import XGBRegressor
import warnings

warnings.filterwarnings("ignore")

# Load your dataset
# Example: Use a CSV file like the Kaggle House Prices dataset
data = pd.read_csv('dataset.csv')

# Basic preprocessing
data = data.dropna(axis=1, thresh=0.8*len(data))  # Drop columns with >20% missing
data = data.fillna(data.median(numeric_only=True))  # Fill missing with median

# Convert categorical variables using one-hot encoding
data = pd.get_dummies(data, drop_first=True)

# Separate features and target
X = data.drop('SalePrice', axis=1)
y = data['SalePrice']

# Feature scaling
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Split into train and test sets
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)

# Initialize smart regression model - XGBoost
xgb = XGBRegressor(objective='reg:squarederror', random_state=42)

# Grid search for hyperparameter tuning (basic example)
param_grid = {
    'n_estimators': [100, 200],
    'max_depth': [3, 5],
    'learning_rate': [0.05, 0.1],
    'subsample': [0.8, 1.0]
}

grid_search = GridSearchCV(estimator=xgb, param_grid=param_grid, cv=3, scoring='neg_root_mean_squared_error', n_jobs=-1)
grid_search.fit(X_train, y_train)

# Best model
best_model = grid_search.best_estimator_

# Predictions
y_pred = best_model.predict(X_test)

# Evaluation
rmse = np.sqrt(mean_squared_error(y_test, y_pred))
print(f"Root Mean Squared Error: {rmse:.2f}")


import pandas as pd
import numpy as np
from sklearn.model_selection import KFold, cross_val_score
from sklearn.metrics import mean_squared_error
from sklearn.linear_model import RidgeCV
from sklearn.ensemble import StackingRegressor
import xgboost as xgb
import lightgbm as lgb

# Load your dataset
df = pd.read_csv("house_data.csv")  # Replace with your file
X = df.drop("SalePrice", axis=1)
y = df["SalePrice"]

# Preprocess data (fill missing values, encoding)
X = X.select_dtypes(include=[np.number]).fillna(0)  # Simplified
# You can add more sophisticated imputation or preprocessing

# Define base models
ridge = RidgeCV(alphas=np.logspace(-3, 3, 13))
xgb_model = xgb.XGBRegressor(n_estimators=100, learning_rate=0.05, max_depth=3)
lgb_model = lgb.LGBMRegressor(n_estimators=100, learning_rate=0.05)

# Stacking ensemble
stack = StackingRegressor(
    estimators=[
        ('ridge', ridge),
        ('xgb', xgb_model),
        ('lgb', lgb_model)
        ],
    final_estimator=RidgeCV()
)

# Cross-validation
kf = KFold(n_splits=5, shuffle=True, random_state=42)
scores = cross_val_score(stack, X, y, cv=kf, scoring='neg_root_mean_squared_error')
print(f"Average RMSE: {-np.mean(scores):.2f}")

# Train final model
stack.fit(X, y)

# Prediction (example)
X_test = pd.read_csv("house_test.csv")  # if you have test data
X_test = X_test.select_dtypes(include=[np.number]).fillna(0)
predictions = stack.predict(X_test)
